import type { AFetch } from '@ai/tools'
import type { ChatRequest, ChatResponse, LLMConfig } from './llm.types'
import { afetchBase } from '@ai/tools'
import { ChatRequestSchema, LLMConfigSchema } from './llm.types'
import { AsyncIteratorCancel, formatMessages, streamToJson } from './llm.util'

/**
 * å¯¹å…¼å®¹ openai v1æ¥å£çš„LLMå°è£…ç±»
 * @see https://docs.ollama.com/api/openai-compatibility#%2Fv1%2Fchat%2Fcompletions
 * @see https://platform.openai.com/docs/api-reference/chat/create
 */
export class LLM {
  /** LLMé…ç½® */
  protected configs: LLMConfig
  /** HTTPè¯·æ±‚ */
  protected fetch: AFetch
  /** å½“å‰æ­£åœ¨è¿›è¡Œçš„æµå¼è¯·æ±‚åˆ—è¡¨ï¼Œç”¨äºç®¡ç†å’Œä¸­æ­¢è¯·æ±‚ */
  protected streamingRequests: Set<AsyncIteratorCancel<object>> = new Set()

  constructor(llmConf: LLMConfig) {
    try {
      this.configs = LLMConfigSchema.parse(llmConf)
      this.fetch = afetchBase({ baseURL: this.configs.baseURL, debug: llmConf.debug })
    }
    catch (error) {
      console.error('ğŸš€ LLMConfig å‚æ•°é”™è¯¯:', error)
      throw error
    }
  }

  /** é‡è½½1: æµå¼å“åº” */
  chat(
    request: ChatRequest & { stream: true },
  ): Promise<AsyncIteratorCancel<ChatResponse>>
  /** é‡è½½2: éæµå¼å“åº” */
  chat(request: ChatRequest & { stream?: false }): Promise<ChatResponse>

  /** è°ƒç”¨OpenAI API èŠå¤©æ¥å£ */
  async chat(request: ChatRequest) {
    /** è¯·æ±‚å‚æ•° */
    const querys = ChatRequestSchema.parse(request)
    if (querys.messages) {
      querys.messages = formatMessages(querys.messages)
    }
    if (querys.stream) {
      return this.fetchStreamRequest('/api/chat', request)
    }
    else {
      return this.fetchRequest('/api/chat', request)
    }
  }

  /** æ™®é€šapiè¯·æ±‚ */
  async fetchRequest(url: string, options: ChatRequest): Promise<ChatResponse> {
    return await this.fetch<ChatResponse>(url, { method: 'POST', data: options })
  }

  /** æµå¼apiè¯·æ±‚, ä¸­é€”å¯å–æ¶ˆ */
  async fetchStreamRequest(url: string, options: ChatRequest): Promise<AsyncIteratorCancel<ChatResponse>> {
    const abortControl = new AbortController()
    const response = await this.fetch(url, {
      signal: abortControl.signal,
      toJson: false,
      method: 'POST',
      data: options,
    })
    if (!response.body) {
      throw new Error('Missing body')
    }
    const jsonStream = streamToJson<ChatResponse>(response.body)
    const abortItr = new AsyncIteratorCancel(abortControl, jsonStream, () => {
      this.streamingRequests.delete(abortItr)
    })
    this.streamingRequests.add(abortItr)
    return abortItr
  }
}
