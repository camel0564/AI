import type { ChatRequest } from './llm.types'
import { isString } from '@ai/tools'

/**
 * æ ¼å¼åŒ–æ¶ˆæ¯ä¸ºå„ç§æ¨¡å‹éœ€è¦çš„æ ¼å¼
 */
export function formatMessages(msgs: ChatRequest['messages']): any[] {
  return msgs.map((message) => {
    let content = ''
    if (isString(message.content)) {
      content = message.content
    }
    else {
      content = JSON.stringify(message.content)
    }
    return {
      role: message.role,
      content,
    }
  })
}

/**
 * è§£æå¤§æ¨¡å‹æµå¼è¿”å›çš„ JSON å­—ç¬¦ä¸²,å¹¶è¿”å›ä¸€ä¸ªå¼‚æ­¥è¿­ä»£å™¨
 */
export async function* streamToJson<T = any>(stream: ReadableStream<Uint8Array>): AsyncGenerator<T> {
  /**
   * ç¼“å†²åŒº
   * æç«¯æƒ…å†µä¸‹: å¤§æ¨¡å‹æµå¼è¿”å›çš„jsonå¯èƒ½ä¸å®Œæ•´ï¼Œéœ€è¦æ‹¼æ¥èµ·æ¥;
   * æ¯æ¬¡è¯»å–åˆ°çš„ chunk å¯èƒ½ä¸æ˜¯ä¸€ä¸ªå®Œæ•´çš„ JSON å¯¹è±¡ï¼Œå¤§æ¨¡å‹ä»¥æ¢è¡Œç¬¦ä¸ºä¸€ä¸ªå®Œæ•´jsonçš„æ ‡å¿—;
   */
  let buffer = ''
  /** ç”¨äºå°† chunk è½¬æ¢ä¸ºå­—ç¬¦ä¸² */
  const decoder = new TextDecoder()
  for await (const chunk of stream) {
    // è¯»å–ä¸‹ä¸€ä¸ªæ•°æ®å—(chunk)
    buffer += decoder.decode(chunk, { stream: true })
    // å°†ç¼“å†²åŒºæŒ‰jsonç»“æŸæ ‡è®°(æ¢è¡Œç¬¦)åˆ†å‰²ï¼Œæ¯è¡Œå°±æ˜¯ä¸€ä¸ªå®Œæ•´çš„ JSON å¯¹è±¡
    const parts = buffer.split('\n')
    buffer = parts.pop() ?? '' // å‰©ä½™çš„å†…å®¹ä½œä¸ºä¸‹ä¸€æ¬¡å¾ªç¯çš„ç¼“å†²åŒº(ä¸‹ä¸€è¡Œçš„éƒ¨åˆ†jsonå­—ç¬¦ä¸²)
    // è§£æå½“å‰æµè¿”å›çš„å®Œæ•´json
    for (const part of parts) {
      try {
        const json = JSON.parse(part) as T // è§£æ JSON å¹¶ç”Ÿæˆç»“æœ
        // console.log('ğŸš€ json:', json)
        yield json
      }
      catch (_error) {
        console.warn('invalid json: ', part)
      }
    }
  }
}

/**
 * å¸¦å–æ¶ˆåŠŸèƒ½ `abort` çš„å¼‚æ­¥è¿­ä»£å™¨
 */
export class AsyncIteratorCancel<T extends object> {
  private readonly abortController: AbortController
  private readonly itr: AsyncGenerator<T | ErrorConstructor>
  private readonly doneCallback: () => void

  constructor(
    abortController: AbortController,
    itr: AsyncGenerator<T | ErrorConstructor>,
    doneCallback: () => void,
  ) {
    this.abortController = abortController
    this.itr = itr
    this.doneCallback = doneCallback
  }

  abort() {
    this.abortController.abort()
  }

  async* [Symbol.asyncIterator]() {
    for await (const message of this.itr) {
      if ('error' in message) {
        throw new Error(message.error as string)
      }
      yield message
      if ((message as any).done || (message as any).status === 'success') {
        this.doneCallback()
        return
      }
    }
    throw new Error('Did not receive done or success response in stream.')
  }
}

/**
 * æŒ‰å­—ç¬¦ä¸²é•¿åº¦åŠ¨æ€è®¡ç®— max_tokens
 * æ ¸å¿ƒé€»è¾‘ï¼šmin(max(300, å­—ç¬¦ä¸²é•¿åº¦//3), 4000)
 * @param inputText è¾“å…¥æ–‡æœ¬
 */
function autoMaxTokens(inputText: string) {
  // 1. è·å–è¾“å…¥æ–‡æœ¬çš„å­—ç¬¦ä¸²é•¿åº¦ï¼ˆæ›¿ä»£ tokens è®¡ç®—ï¼‰
  const inputLength = inputText.length

  // 2. æ ¸å¿ƒé€»è¾‘ï¼šå’Œ Python ç­‰ä»·ï¼ˆ// å¯¹åº” Math.floor æ•´æ•°é™¤æ³•ï¼‰
  const baseValue = Math.floor(inputLength / 3) // å¯¹åº” input_tokens // 3
  const dynamicValue = Math.max(300, baseValue) // æœ€ä½ 300
  const finalMaxTokens = Math.min(dynamicValue, 4000) // æœ€é«˜ 4000

  return {
    /** æœ€å¤§è¾“å‡º token æ•° */
    max_tokens: finalMaxTokens,
    /** å…¼å®¹: openai oç³»åˆ—æ¨ç†æ¨¡å‹ */
    max_completion_tokens: finalMaxTokens,
  }
}
