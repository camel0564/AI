/**
 * OpenAI API å…¼å®¹æ¨¡åž‹
 */

import type { Message as ClientMessage } from 'ollama/src/interfaces.js'
import type { ChatConfig, LLMConfig } from './llm.types.js'
import process from 'node:process'
import { isString } from '@ai/tools'
import { Ollama as Client } from 'ollama'
import { ChatConfigSchema, LLMConfigSchema } from './llm.types.js'

export class LLM {
  /** OpenAIå®¢æˆ·ç«¯ */
  protected client: Client
  /** LLMé…ç½® */
  protected configs: LLMConfig

  constructor(llmConf: LLMConfig) {
    try {
      this.configs = LLMConfigSchema.parse(llmConf)
      console.log('ðŸš€ this.configs:', this.configs)
      this.client = this.initOpenAI(this.configs)
    }
    catch (error) {
      console.error('ðŸš€ LLMConfig å‚æ•°é”™è¯¯:', error)
      throw error
    }
  }

  /** åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ */
  initOpenAI(llmConf: LLMConfig) {
    const { apiKey, baseURL } = llmConf

    // ç¡®ä¿APIå¯†é’¥å·²è®¾ç½®
    const key = apiKey || process.env.OPENAI_API_KEY

    // åˆ›å»ºOpenAIå®¢æˆ·ç«¯
    const ollama = new Client({
      host: baseURL,
      headers: key ? { Authorization: `Bearer ${key}` } : undefined,
    })
    return ollama
  }

  /** è°ƒç”¨OpenAI API èŠå¤©æŽ¥å£ */
  async chat(chatConf: ChatConfig) {
    const { messages, functions } = ChatConfigSchema.parse(chatConf)
    const model = this.configs.model
    const msgs = this.formatMessages(messages)
    console.log('ðŸš€ msgs:', model, msgs)
    const response = await this.client.chat({
      model,
      messages: msgs,
      stream: true,
    })
    console.log('ðŸš€ response:', response)
    for await (const part of response) {
      process.stdout.write(part.message.content || part.message.thinking || '...')
    }
  }

  formatMessages(messages: ChatConfig['messages']): ClientMessage[] {
    return messages.map((message) => {
      let content = ''
      if (isString(message.content)) {
        content = message.content
      }
      else {
        content = JSON.stringify(message.content)
      }
      return {
        role: message.role,
        content,
      }
    })
  }
}
