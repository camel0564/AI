/**
 * OpenAI API å…¼å®¹æ¨¡å‹
 */

import type { ChatConfig, LLMConfig } from './llm.types'
import process from 'node:process'
import Client from 'openai'
import { ChatConfigSchema, LLMConfigSchema } from './llm.types'

export class LLM {
  /** OpenAIå®¢æˆ·ç«¯ */
  protected client: Client
  /** LLMé…ç½® */
  protected configs: LLMConfig

  constructor(llmConf: LLMConfig) {
    try {
      this.configs = LLMConfigSchema.parse(llmConf)
      console.log('ğŸš€ this.configs:', this.configs)
      this.client = this.initOpenAI(this.configs)
    }
    catch (error) {
      console.error('ğŸš€ LLMConfig å‚æ•°é”™è¯¯:', error)
      throw error
    }
  }

  /** åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ */
  initOpenAI(llmConf: LLMConfig) {
    const { apiKey, baseURL } = llmConf

    // ç¡®ä¿APIå¯†é’¥å·²è®¾ç½®
    const key = apiKey || process.env.OPENAI_API_KEY

    // åˆ›å»ºOpenAIå®¢æˆ·ç«¯
    return new Client({
      apiKey: key,
      baseURL,
    })
  }

  /** è°ƒç”¨OpenAI API èŠå¤©æ¥å£ */
  async chat(chatConf: ChatConfig) {
    const { messages, functions } = ChatConfigSchema.parse(chatConf)

    const request = {
      model: this.configs.model,
      input: messages,
      tools: functions,
    } as any
    console.log('ğŸš€ request:', request)
    const response = await this.client.responses.create(request)
    console.log('ğŸš€ response:', response)
  }
}
